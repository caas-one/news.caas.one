# Kubernetes基础设施-基础设施组件
##   概述
在OKD中，Kubernetes管理一组容器或主机上的容器化应用程序，并提供部署、维护和应用程序缩放的机制。容器运行时包，实例化，并运行容器化的应用程序。Kubernetes集群由一个或多个主节点和一组节点组成。
您可以选择为高可用性（HA）配置主服务器，以确保集群没有单点故障。
> OKD使用Kubernetes 1.11和Docker 1.13.1。
##   主机（Master）
主机是包含控制平面组件的主机或主机，包括API服务器、控制器管理器服务器和ETCD。master管理其kubernetes集群中的节点，并安排 [pods](https://docs.okd.io/latest/architecture/core_concepts/pods_and_services.html#pods) 在这些节点上运行。
+ *表 1.主组件*

| 成分（Component）  | 描述（Description）|
|------------------------------|----------------------------|
|API服务器                   |           Kubernetes API服务器验证和配置pod、服务和复制控制器的数据。它还将pod分配给节点，并将pod信息与服务配置同步。|
|ETCD                          |           ETCD会存储持久的主状态，而其他组件则监视ETCD的更改以使自己进入所需的状态。ETCD可以选择性地配置为高可用性，通常与2n+1对等服务一起部署。|
|控制器管理服务器(Controller Manager Server)             |         控制器管理器服务器监视ETCD以改变复制控制器对象，然后使用API来执行所需的状态。几个这样的过程创建一个集群与一个活跃的领导者在同一时间。|
|HAProxy                      |可选的，当使用本机方法配置高度可用的主机时，可以平衡API主端点之间的负载。群集[安装过程 ](https://docs.okd.io/latest/install/index.html#install-planning) 。可以用原生方法为HAProxy配置。或者，您可以使用本机方法，但可以预先配置自己的负载均衡器。
###         控制平面静态吊舱（Control Plane Static Pods）
核心控制平面组件、API服务器和控制器管理器组件作为由Kubelet操作的 [*static pods*](https://kubernetes.io/docs/tasks/administer-cluster/static-pod/) 运行。
对于在同一主机上有ETCD的主机，ETCD也被移动到静态播客。基于RPM的ETCD仍然在不是主机的ETCD主机上受支持。
此外，节点组件 **openshift-sdn** 和 **openvswitch** 现在使用守护进程而不是 **systemd** 服务运行。
![图1. 控制平面主机架构更改](https://img-blog.csdnimg.cn/20191004175433986.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1doaXNwZXJfV2FuZw==,size_16,color_FFFFFF,t_70)
+ ( *图1. 控制平面主机架构更改* )
即使控制平面组件作为静态POD运行，主主机仍然从 **/etc/origin/master/master-config.yaml** 文件中获取配置,如[主节点配置主题](../../install_config/master_node_configuration.html#install-configmaster-node-configuration)中所述。
#### MIRROR PODS
主节点上的Kubelet自动在API服务器上为每个控制平面静态POD创建镜像POD，以便它们在 **kube-system** 项目的集群中可见。默认情况下，这些静态POD的清单由 **openshift-ansible** 安装程序安装，该安装程序位于主主机上的 **/etc/origin/node/pods** 目录中。
这些pod定义了以下hostPath volumes:
|     |      |
|--------|------|
| **/etc/origin/master**| 包含所有证书、配置文件和 **admin.kubeconfig** 文件。|
| **/var/lib/origin**       |包含二进制文件的volume和潜在的核心转储。|
| **/etc/origin/cloudprovide** |包含特定于云提供商的配置（AWS、Azure等）。|
| **/usr/libexec/kubernetes/kubelet-plugins** |包含其他第三方volume插件。|
| **/etc/origin/kubelet-plugins** |包含用于系统容器的其他第三方volume插件。|
可以在静态播客上执行的操作集是有限的。例如
```
       $ oc logs master-api-<hostname> -n kube-system
```
返回API服务器的标准输出。然而：
```
        $ oc delete pod master-api-<hostname> -n kube-system
```
不会真正删除pod。
另一个例子是，集群管理员可能希望执行一个常见的操作，例如增加API服务器的 *loglevel* ，以便在出现问题时提供更详细的数据。必须编辑 */etc/origin/master/master.env* 文件，在该文件中，可以修改 *OPTIONS* 变量中的 *loglevel* 参数，因为该值将传递给在容器中运行的进程。更改需要重新启动在容器内运行的进程。
#### 重新启动主服务
要重新启动在控制平面静态播客中运行的控制平面服务，请在主主机上使用 *master-restart* 命令。
要重新启动主API，请执行以下操作：
```
        # master-restart api
```
要重新启动控制器：
```
        # master-restart controllers
```
要重新启动ETCD:
```
        # master-restart etcd
```
#### 查看主服务日志
要查看在控制平面静态播客中运行的控制平面服务的日志，请使用相应组件的 *master-logs* 命令：
```
       # master-logs api api 
       # master-logs controllers controllers 
       # master-logs etcd etcd
```
###         高可用性主机（High Availability Masters）
您可以选择为高可用性（HA）配置主服务器，以确保集群没有单点故障。
为了减轻对主服务器可用性的担忧，建议执行两项活动：
1. 应该创建一个[Runbook](https://en.wikipedia.org/wiki/Runbook)条目来重建主控形状。Runbook条目是任何高可用性服务的必要支持。附加的解决方案只控制必须查阅Runbook的频率。例如，主主机的冷备用可以充分满足SLA要求，即创建新应用程序或恢复失败的应用程序组件所需的停机时间不超过分钟。
2. 使用高可用性解决方案配置主服务器，并确保群集没有单点故障。[群集安装文档](https://docs.okd.io/latest/install/example_inventories.html#multiple-masters)提供了使用 *native* HA方法和配置HAProxy的特定示例。您还可以使用 *native* 方法而不是HAProxy，将这些概念应用到现有的HA解决方案中。
>在生产OKD集群中，必须保持API服务器负载均衡器的高可用性。如果API服务器负载平衡器不可用，则节点无法报告其状态，其所有pPOD都被标记为死区，POD的端点将从服务中移除。
>除了为OKD配置HA之外，还必须为API服务器负载平衡器单独配置HA。要配置HA，最好集成企业负载均衡器（LB），如F5 Big-IP™ 或 Citrix Netscaler™ 设备。如果这些解决方案不可用，则可以运行多个HAProxy负载平衡器，并使用Keepalived为HA提供浮动虚拟IP地址。但是，不建议将此解决方案用于生产实例。

当将 *native* HA方法与HAProxy一起使用时，主组件具有以下可用性：
+ *表 2.具有HAProxy的可用性矩阵(Availability Matrix)*

|    角色          |      风格     |      笔记             |
|---------------|-----------------|--------------|
| ETCD   |主动——主动|具有负载平衡的完全冗余部署。可以安装在单独的主机上，也可以并置在主主机上。|
|API服务器|主动——主动|由HAProxy管理。|
|控制器管理器服务器|主动——被动|一次选择一个实例作为集群领导。|
|HAProxy|主动——被动|在API主端点之间平衡负载。|
虽然集群ETCD需要奇数个主机作为仲裁，但主服务没有仲裁，也不要求它们具有奇数个主机。但是，由于HA至少需要两个主服务，因此在配置主服务和ETCD时，通常保持主机的统一奇数。
##   结点
节点为容器提供运行时环境。Kubernetes集群中的每个节点都有需要由主节点管理的服务。节点还具有运行POD所需的服务，包括容器运行时、Kubelet和服务代理。
OKD从云提供商、物理系统或虚拟系统创建节点。Kubernetes与表示这些节点的节点对象交互。主节点使用来自节点对象的信息来通过运行状况检查验证节点。节点在通过运行状况检查之前将被忽略，而主节点将继续检查节点，直到它们有效。[Kubernetes文档](https://kubernetes.io/docs/concepts/architecture/nodes/management)提供了有关节点状态和管理的更多信息。
管理员可以使用CLI管理OKD实例中的[节点](https://docs.okd.io/latest/admin_guide/manage_nodes.html#admin-guide-manage-nodes) 。要在启动节点服务器时定义完整配置和安全选项，请使用[专用节点配置文件](https://docs.okd.io/latest/install_config/master_node_configuration.html#install-config-master-node-configuration) 。
>有关建议的最大节点数，请参阅[群集限制](.https://docs.okd.io/latest/scaling_performance/cluster_limits.html#scaling-performance-cluster-limits)部分。
###         Kubelet
每个节点都有一个Kubelet，它按照容器清单（一个描述POD的YAML文件）的指定更新节点。Kubelet使用一组清单来确保其容器已启动并继续运行。
集装箱舱单可通过以下方式提供给Kubelet：
* 命令行上每20秒检查一次的文件路径。
* 在命令行上传递的每20秒检查一次的HTTP端点。
* Kubelet监视ETCD服务器，如 **/registry/hosts/$（hostname -f）** ，并对任何更改执行操作。
* Kubelet监听HTTP并响应一个简单的API来提交一个新的清单。
###         服务代理（Service Proxy）
每个节点还运行一个简单的网络代理，该代理反映在该节点上的API中定义的服务。这允许节点跨一组后端执行简单的TCP和UDP流转发。
####         节点对象定义（Node Object Deﬁnition）
以下是kubernetes中的节点对象定义示例：
```
apiVersion: v1
kind: Node
metadata: 
  creationTimestamp: null 
  labels: 
    kubernetes.io/hostname: node1.example.com
  name: node1.example.com
spec:
  externalID: node1.example.com
status: 
  nodeInfo: 
     bootID: "" 
     containerRuntimeVersion: "" 
     kernelVersion: "" 
     kubeProxyVersion: "" 
     kubeletVersion: "" 
     machineID: "" 
     osImage: "" 
     systemUUID: ""
```
1.  *apiVersion* 定义要使用的API版本。
2. *kind* 设置*node* 将此标识为节点对象的定义。
3.  *metadata.labels* 列出已添加到节点的任何[标签](https://docs.okd.io/latest/architecture/core_concepts/pods_and_services.html#labels)。
4.  *metadata.name*  是定义节点对象名称的必需值。当运行 *oc get nodes* 命令时，该值显示在NAME列中。
5. *spec.external ID* 定义可以访问节点的完全限定域名。空时默认为 *metadata.name* 值。
###         节点引导（Node Bootstrapping）
节点的配置是从主节点引导的，这意味着节点从主节点拉取其预定义的配置以及客户机和服务器证书。这允许通过减少节点之间的差异更快地启动节点，以及集中更多的配置并让集群聚合到所需的状态。默认情况下启用证书循环和集中证书管理。
![图2.节点引导工作流概述](https://img-blog.csdnimg.cn/20191004175245101.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1doaXNwZXJfV2FuZw==,size_16,color_FFFFFF,t_70)
+ （*图2.节点引导工作流概述* ）
当节点服务启动时，节点在加入集群之前检查 **/etc/origin/node/node.kubeconfig** 文件和其他节点配置文件是否存在。如果没有，则节点从主节点提取配置，然后加入集群。[Configmaps](.https://docs.okd.io/latest/dev_guide/configmaps.html#dev-guide-configmaps)用于在集群中存储节点配置，该集群在节点主机上的 **/etc/origin/node/node-config.yaml** 处填充配置文件。有关默认节点组及其配置映射集的定义，请参阅安装群集中的[定义节点组和主机映射](https://docs.okd.io/latest/install/configuring_inventory_file.html#configuring-inventory-defining-node-group-and-host-mappings)。
#### 节点引导工作流（NODE BOOTSTRAP WORKFLOW）
自动节点引导过程使用以下工作流：
1. 默认情况下，在群集安装期间，会创建一组 *ClusterRole* 、 *ClusterRoleBinding* 和 *ServiceAccount* 对象，以用于节点引导：
- 系统：节点引导程序群集角色用于在节点引导期间创建证书签名请求（CSR）：
```
# oc describe clusterrole.authorization.openshift.io/syste

Name:                    system:node-bootstrapper 
Created:                 17 hours ago 
Labels:                  kubernetes.io/bootstrapping=rbac-d 
Annotations:             authorization.openshift.io/system  
                         openshift.io/reconcile-protect=fal 
Verbs                    Non-Resource URLs               Resource N 
[create get list watch]  []                              []
```
- 以下节点引导程序服务帐户是在openshift infra项目中创建的：
```
# oc describe sa node-bootstrapper -n openshift-infra
 
 Name:                node-bootstrapper 
 Namespace:           openshift-infra 
 Labels:              <none> 
 Annotations:         <none>
 Image pull secrets:  node-bootstrapper-dockercfg-f2n8r 
 Mountable secrets:   node-bootstrapper-token-79htp   
                      node-bootstrapper-dockercfg-f2n8r 
Tokens:               node-bootstrapper-token-79htp 
                      node-bootstrapper-token-mqn2q
Events:               <none>
```
- 以下 **系统：system:node-bootstrapper** 群集角色绑定用于节点引导程序群集角色和服务帐户：
```
# oc describe clusterrolebindings system:node-bootstrapper

Name:                        system:node-bootstrapper 
Created:                     17 hours ago 
Labels:                      <none> 
Annotations:                 openshift.io/reconcile-protect=fal
Role:                        /system:node-bootstrapper 
Users:                       <none> 
Groups:                      <none> 
ServiceAccounts:            openshift-infra/node-bootstrapper 
Subjects:                    <none> 
Verbs                        Non-Resource URLs           Resource N 
[create get list watch]      []                          [] 
```
2. 默认情况下，在集群安装期间， **openshift ansibe** 安装程序会在 **/etc/origin/master** 目录中创建一个OKD证书颁发机构和各种其他证书、密钥和 **kubeconfig** 文件。值得注意的两个文件是：

|              |                 |
|------------|-----------|
| /etc/origin/master/admin.kubeconfig | 使用 **system:admin** user. |
|/e t c /o rigin / mas t e r/ b o o t s t r ap.k u b e c o n fig|用于主节点以外的节点引导。|
a. 在安装程序使用节点引导程序服务帐户时创建，如下所示：
```
$ oc --config=/etc/origin/master/admin.kubeconfig \ 
    serviceaccounts create-kubeconfig node-bootstrapper \
    -n openshift-infra
```
b. 在主节点上，使用
 **/etc/origin/master/admin.kubeconfig** 作为引导文件，并将其复制到
 **/etc/origin/node/bostrap.kubeconfig** 。在另一个非主节点上，
 **/etc/origin/master/bootsrapp.kubeconfig** 文件被复制到每个节点主机上在
 **/etc/origin/node/boostrapp.kubeconfig** 中的所有其他节点。
```
--bootstrap-kubeconfig=/etc/origin/node/bootstrap.kubeconf
```
3. Kubelet首先使用提供的 **/etc/origin/node/botstrap.kubeconfig** 文件启动。在内部初始连接之后，Kubelet创建证书签名请求（CSRs）并将它们发送到主机。
4. CSR通过控制器管理器（特别是证书签名控制器）进行验证和批准。如果批准，Kubelet客户机和服务器证书将在 **/etc/origin/node/ceritificates** 目录中创建。例如：
```
# ls -al /etc/origin/node/certificates/ 
total 12 
drwxr-xr-x. 2 root root  212 Jun 18 21:56 . 
drwx------. 4 root root  213 Jun 19 15:18 .. 
-rw-------. 1 root root 2826 Jun 18 21:53 kubelet-client-2018-06 
-rw-------. 1 root root 1167 Jun 18 21:53 kubelet-client-2018-06 
lrwxrwxrwx. 1 root root   68 Jun 18 21:53 kubelet-client-current
-rw-------. 1 root root 1447 Jun 18 21:56 kubelet-server-2018-06
lrwxrwxrwx. 1 root root   68 Jun 18 21:56 kubelet-server-current
```
5. CSR批准后，在 **/etc/origin/node/nodekubeconfig** 创建 **nodekubeconfig** 文件。              
6. Kubelet使用 **/etc/origin/node/node.kubeconfig** 文件和 **/etc/origin/node/certificates/** 目录中的证书重新启动，然后准备加入集群。
#### 节点配置工作流（NODE CONFIGURATION WORKFLOW）
寻源节点的配置使用以下工作流：
1. 最初，节点的Kubelet由引导配置文件 **botstrap-node-config.yaml** 启动，该文件位于 **/etc/origin/node/** 目录中，在节点配置时创建。
2. 在每个节点上，节点服务文件使用 **/usr/local/bin/** 目录中的本地脚本 **openshift-node** 以提供的 **botstrap-node-config.yaml** 启动Kubelet。
3. 在每个主机上， **/etc/origin/node/pods** 目录包含 **apiserver** 、**controller** 和** etcd** 的POD清单，这些清单是作为主机上的静态POD创建的。              
4. 在集群安装期间，会创建一个同步守护程序集，该守护程序会在每个节点上创建一个同步POD。Sync POD监视文件 **/etc/sy-sconfig/atomic-oppenhift-node** 中的更改。它专门监视要设置的 *BOOTSTRAP_CONFIG_NAME* 。*BOOTSTRAP_CONFIG_NAME* 由 **openshift-ansible** 安装程序设置，是基于节点所属的节点配置组的ConfigMap的名称。

默认情况下，安装程序创建以下节点配置组：
+ node-config-master
+ node-config-infra
+ node-config-compute
+ node-config-all-in-one
+ node-config-master-infra
在 **openshift node** 项目中为每个组创建一个ConfigMap。
5. sync pod根据 *BOOTSTRAP_CONFIG_NAME* 中设置的值提取适当的ConfigMap。
6. sync pod将ConfigMap数据转换为Kubelet配置，并为该节点主机创建**/etc/origin/node/nodeconfig.yaml** 。如果对此文件进行了更改（或它是文件的初始创建），则将重新启动Kubelet。
#### 修改节点配置（MODIFYING NODE CONFIGURATIONS）
通过在 **openshift** 节点项目中编辑适当的ConfigMap来修改节点的配置。不能直接修改 **/etc/origin/node/node-config.yaml** 。 
例如，对于 **node-config-compute** 组中的节点，使用以下命令编辑ConfigMap：
```
$ oc edit cm node-config-compute -n openshift-node
```
